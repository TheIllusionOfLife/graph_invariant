"""Report and figure data writers for graph invariant analysis."""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any

from experiment_analysis import (
    _ast_node_count,
    _get_spearman,
    _normalize_candidate_code_for_report,
    build_appendix_payload,
    build_comparison_table,
    build_seed_aggregates,
)

from graph_invariant.stats_utils import mean_std_ci95, safe_float  # noqa: F401

# ── LaTeX helpers ─────────────────────────────────────────────────────


def _fmt_tex_float(value: Any, digits: int = 3) -> str:
    numeric = safe_float(value)
    if numeric is None:
        return "N/A"
    return f"{numeric:.{digits}f}"


def _escape_latex_text(value: Any) -> str:
    """Escape LaTeX special characters in untrusted text."""
    text = str(value)
    replacements = [
        ("\\", "\\textbackslash{}"),
        ("&", "\\&"),
        ("%", "\\%"),
        ("$", "\\$"),
        ("#", "\\#"),
        ("_", "\\_"),
        ("{", "\\{"),
        ("}", "\\}"),
        ("~", "\\textasciitilde{}"),
        ("^", "\\textasciicircum{}"),
    ]
    for source, target in replacements:
        text = text.replace(source, target)
    return text


def _escape_latex_name(name: str) -> str:
    """Escape a path-like experiment name for LaTeX with line-break hints.

    Adds ``\\allowbreak`` after every ``\\_`` and ``/`` so that LaTeX can
    wrap long experiment paths inside narrow table columns without producing
    Overfull \\hbox warnings.
    """
    text = _escape_latex_text(name)
    return text.replace("/", "/\\allowbreak ").replace("\\_", "\\_\\allowbreak ")


def write_appendix_tables_tex(appendix_payload: dict[str, Any], output_path: Path) -> None:
    """Write generated appendix tables as LaTeX source."""
    seed_aggregates = appendix_payload.get("appendix_seed_aggregates", {})
    seed_notes = appendix_payload.get("appendix_seed_notes", {})
    bounds = appendix_payload.get("appendix_bounds_diagnostics", {})
    runtimes = appendix_payload.get("appendix_runtime_summary", {})

    caption = "Seed-aggregated performance for NeurIPS matrix runs."
    if isinstance(seed_notes, dict) and seed_notes:
        caption += (
            " \\textsuperscript{\\dag}Groups marked with \\textsuperscript{\\dag} "
            "have incomplete seeds."
        )

    lines: list[str] = [
        "% Auto-generated by analysis/analyze_experiments.py. Do not edit manually.",
        "",
        "\\subsection{Multi-seed aggregate metrics}",
        "\\begin{table}[h]",
        "  \\centering",
        "  \\small",
        f"  \\caption{{{caption}}}",
        "  \\label{tab:appendix_seed_aggregates}",
        "  \\begin{tabular}{p{4.6cm}ccccc}",
        "    \\toprule",
        "    Group & Seeds & Success & Val mean$\\pm$std & Test mean$\\pm$std & Test CI95 \\\\",
        "    \\midrule",
    ]

    if isinstance(seed_aggregates, dict) and seed_aggregates:
        for group, payload in sorted(seed_aggregates.items()):
            val = payload.get("val_spearman", {})
            test = payload.get("test_spearman", {})
            seed_count = payload.get("seed_count", 0)
            success_count = payload.get("success_count", 0)
            lines.append(
                "    {group} & {seed_count} & {success_count}/{seed_count} & "
                "{val_mean}$\\pm${val_std} & {test_mean}$\\pm${test_std} "
                "& $\\pm${test_ci} \\\\".format(
                    group=(
                        _escape_latex_name(group)
                        + ("\\textsuperscript{\\dag}" if group in seed_notes else "")
                    ),
                    seed_count=seed_count,
                    success_count=success_count,
                    val_mean=_fmt_tex_float(val.get("mean"), 3),
                    val_std=_fmt_tex_float(val.get("std"), 3),
                    test_mean=_fmt_tex_float(test.get("mean"), 3),
                    test_std=_fmt_tex_float(test.get("std"), 3),
                    test_ci=_fmt_tex_float(test.get("ci95_half_width"), 3),
                )
            )
    else:
        lines.append("    \\multicolumn{6}{c}{No seed aggregates found.} \\\\")

    lines.extend(
        [
            "    \\bottomrule",
            "  \\end{tabular}",
            *(
                [
                    "  \\vspace{2pt}",
                    "  \\raggedright\\footnotesize "
                    + " \\\\ ".join(
                        "\\textsuperscript{\\dag}" + _escape_latex_text(note)
                        for _, note in sorted(seed_notes.items())
                    ),
                ]
                if isinstance(seed_notes, dict) and seed_notes
                else []
            ),
            "\\end{table}",
            "",
            "\\subsection{Bounds diagnostics}",
            "\\begin{table}[h]",
            "  \\centering",
            "  \\small",
            "  \\caption{Validation/test bounds diagnostics for bounds-mode runs.}",
            "  \\label{tab:appendix_bounds}",
            "  \\begin{tabular}{p{5.4cm}cccc}",
            "    \\toprule",
            "    Experiment & Val score & Val sat. & Test score & Test sat. \\\\",
            "    \\midrule",
        ]
    )

    if isinstance(bounds, dict) and bounds:
        for name, payload in sorted(bounds.items()):
            lines.append(
                "    {name} & {vbs} & {vsr} & {tbs} & {tsr} \\\\".format(
                    name=_escape_latex_name(name),
                    vbs=_fmt_tex_float(payload.get("val_bound_score"), 3),
                    vsr=_fmt_tex_float(payload.get("val_satisfaction_rate"), 3),
                    tbs=_fmt_tex_float(payload.get("test_bound_score"), 3),
                    tsr=_fmt_tex_float(payload.get("test_satisfaction_rate"), 3),
                )
            )
    else:
        lines.append("    \\multicolumn{5}{c}{No bounds diagnostics found.} \\\\")

    lines.extend(
        [
            "    \\bottomrule",
            "  \\end{tabular}",
            "\\end{table}",
            "",
            "\\subsection{Runtime and completion summary}",
            "\\begin{table}[h]",
            "  \\centering",
            "  \\small",
            "  \\caption{Runtime summary for matrix runs from matrix\\_summary files.}",
            "  \\label{tab:appendix_runtime}",
            "  \\begin{tabular}{p{4.6cm}cccc}",
            "    \\toprule",
            "    Experiment & Mean sec & Std sec & Completed/Total & Criteria success \\\\",
            "    \\midrule",
        ]
    )

    if isinstance(runtimes, dict) and runtimes:
        for name, payload in sorted(runtimes.items()):
            duration = payload.get("duration_sec", {})
            completed = payload.get("completed_runs", 0)
            total = payload.get("total_runs", 0)
            criteria = payload.get("criteria_success_runs", 0)
            lines.append(
                "    {name} & {mean} & {std} & {completed}/{total} & {criteria} \\\\".format(
                    name=_escape_latex_text(name),
                    mean=_fmt_tex_float(duration.get("mean"), 2),
                    std=_fmt_tex_float(duration.get("std"), 2),
                    completed=completed,
                    total=total,
                    criteria=criteria,
                )
            )
    else:
        lines.append("    \\multicolumn{5}{c}{No runtime summary found.} \\\\")

    lines.extend(
        [
            "    \\bottomrule",
            "  \\end{tabular}",
            "\\end{table}",
            "",
        ]
    )

    # --- Self-correction failure breakdown table ---
    sc_failure_data = appendix_payload.get("sc_failure_breakdown", {})
    lines.extend(
        [
            "\\subsection{Self-correction failure breakdown}",
            "\\begin{table}[h]",
            "  \\centering",
            "  \\small",
            "  \\caption{Self-correction failure categories per experiment."
            " Counts are cumulative event tallies: a single failed attempt may"
            " be counted in multiple categories (e.g., both no valid predictions"
            " and below novelty gate), so column sums can exceed SC attempted."
            " Novelty-gate rejections dominate, confirming the hard gate"
            " is the primary bottleneck rather than code quality.}",
            "  \\label{tab:appendix_sc_failures}",
            "  \\resizebox{\\linewidth}{!}{%",
            "  \\begin{tabular}{p{4.0cm}cccc}",
            "    \\toprule",
            "    Experiment & SC attempted & No valid preds & Below train thr."
            " & Below novelty gate \\\\",
            "    \\midrule",
        ]
    )
    if isinstance(sc_failure_data, dict) and sc_failure_data:
        for name, payload in sorted(sc_failure_data.items()):
            lines.append(
                "    {name} & {attempted} & {no_valid} & {below_train}"
                " & {below_novelty} \\\\".format(
                    name=_escape_latex_name(name),
                    attempted=_escape_latex_text(payload.get("attempted", "N/A")),
                    no_valid=_escape_latex_text(payload.get("no_valid_train_predictions", "N/A")),
                    below_train=_escape_latex_text(payload.get("below_train_threshold", "N/A")),
                    below_novelty=_escape_latex_text(payload.get("below_novelty_threshold", "N/A")),
                )
            )
    else:
        lines.append("    \\multicolumn{5}{c}{No self-correction data found.} \\\\")
    lines.extend(
        [
            "    \\bottomrule",
            "  \\end{tabular}%",
            "  }",
            "\\end{table}",
            "",
        ]
    )

    # --- Compute profile table ---
    compute_data = appendix_payload.get("compute_profile", {})
    lines.extend(
        [
            "\\subsection{Compute profile}",
            "\\begin{table}[h]",
            "  \\centering",
            "  \\small",
            "  \\caption{Estimated LLM call budget per experiment."
            " LLM generation calls = generations $\\times$ islands $\\times$ population."
            " Repair calls = self-correction attempts."
            " Total calls = generation + repair."
            " Estimated at 5--15\\,s per LLM call on a local 20B model.}",
            "  \\label{tab:appendix_compute}",
            "  \\resizebox{\\linewidth}{!}{%",
            "  \\begin{tabular}{p{4.0cm}ccccc}",
            "    \\toprule",
            "    Experiment & Gens & LLM gen calls & Repair calls & Total calls & PySR budget \\\\",
            "    \\midrule",
        ]
    )
    if isinstance(compute_data, dict) and compute_data:
        for name, payload in sorted(compute_data.items()):
            lines.append(
                "    {name} & {gens} & {gen_calls} & {repair} & {total} & {pysr} \\\\".format(
                    name=_escape_latex_name(name),
                    gens=_escape_latex_text(payload.get("generations", "N/A")),
                    gen_calls=_escape_latex_text(payload.get("llm_gen_calls", "N/A")),
                    repair=_escape_latex_text(payload.get("repair_calls", "N/A")),
                    total=_escape_latex_text(payload.get("total_calls", "N/A")),
                    pysr=_escape_latex_text(payload.get("pysr_budget", "60 s")),
                )
            )
    else:
        lines.append("    \\multicolumn{6}{c}{No compute profile data found.} \\\\")
    lines.extend(
        [
            "    \\bottomrule",
            "  \\end{tabular}%",
            "  }",
            "\\end{table}",
            "",
        ]
    )

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text("\n".join(lines) + "\n", encoding="utf-8")


# ── Output functions ─────────────────────────────────────────────────


def write_analysis_report(experiments: dict[str, dict], output_path: Path) -> None:
    """Write a markdown analysis report summarizing all experiments."""
    lines: list[str] = ["# Cross-Experiment Analysis Report", ""]

    table_data = build_comparison_table(experiments)
    if table_data:
        lines.extend(["## Experiment Comparison", ""])
        headers = ["Experiment", "Mode", "Success", "Val Spearman", "Test Spearman", "Generations"]
        lines.append("| " + " | ".join(headers) + " |")
        lines.append("| " + " | ".join(["---"] * len(headers)) + " |")
        for row in table_data:
            val_s = f"{row['val_spearman']:.4f}" if row.get("val_spearman") is not None else "N/A"
            test_s = (
                f"{row['test_spearman']:.4f}" if row.get("test_spearman") is not None else "N/A"
            )
            cells = [
                row["experiment"],
                row.get("fitness_mode", ""),
                str(row.get("success", "")),
                val_s,
                test_s,
                str(row.get("final_generation", "")),
            ]
            lines.append("| " + " | ".join(cells) + " |")
        lines.append("")

    aggregates = build_seed_aggregates(experiments)
    if aggregates:
        lines.extend(["## Multi-Seed Aggregates", ""])
        lines.append(
            "| Experiment Group | Seeds | Val mean±std | Val CI95 | "
            "Test mean±std | Test CI95 | CI clamp |"
        )
        lines.append("| --- | --- | --- | --- | --- | --- | --- |")
        for group, payload in sorted(aggregates.items()):
            val = payload["val_spearman"]
            test = payload["test_spearman"]
            clamp_parts: list[str] = []
            if payload.get("val_ci95_clamped_to_bounds", False):
                clamp_parts.append("val")
            if payload.get("test_ci95_clamped_to_bounds", False):
                clamp_parts.append("test")
            val_mean_std = (
                f"{val['mean']:.4f} ± {val['std']:.4f}" if val.get("mean") is not None else "N/A"
            )
            val_ci = (
                f"±{val['ci95_half_width']:.4f}"
                if val.get("ci95_half_width") is not None
                else "N/A"
            )
            test_mean_std = (
                f"{test['mean']:.4f} ± {test['std']:.4f}" if test.get("mean") is not None else "N/A"
            )
            test_ci = (
                f"±{test['ci95_half_width']:.4f}"
                if test.get("ci95_half_width") is not None
                else "N/A"
            )
            lines.append(
                "| {group} | {seed_count} | {val_mean_std} | {val_ci} | "
                "{test_mean_std} | {test_ci} | {clamp} |".format(
                    group=group,
                    seed_count=payload["seed_count"],
                    val_mean_std=val_mean_std,
                    val_ci=val_ci,
                    test_mean_std=test_mean_std,
                    test_ci=test_ci,
                    clamp="+".join(clamp_parts) if clamp_parts else "none",
                )
            )
        lines.append("")

    for name, data in experiments.items():
        summary = data.get("summary", {})
        convergence = data.get("convergence", {})
        baselines = data.get("baselines", {})
        ood = data.get("ood", {})

        lines.extend([f"## {name}", ""])
        lines.append(f"- Fitness mode: {summary.get('fitness_mode', 'unknown')}")
        lines.append(f"- Success: {summary.get('success', False)}")
        lines.append(f"- Stop reason: {summary.get('stop_reason', 'N/A')}")
        lines.append(f"- Final generation: {summary.get('final_generation', 'N/A')}")

        val_s = _get_spearman(summary.get("val_metrics"))
        test_s = _get_spearman(summary.get("test_metrics"))
        if val_s is not None:
            lines.append(f"- Validation Spearman: {val_s:.4f}")
        if test_s is not None:
            lines.append(f"- Test Spearman: {test_s:.4f}")

        code_nodes = _ast_node_count(summary.get("best_candidate_code"))
        if code_nodes is not None:
            lines.append(f"- Best formula AST nodes: {code_nodes}")

        bounds = data.get("bounds_diagnostics", {})
        if bounds:
            lines.extend(["", "### Bounds Diagnostics", ""])
            for key in sorted(bounds.keys()):
                lines.append(f"- {key}: {bounds[key]}")

        if convergence:
            scores = convergence.get("best_scores", [])
            if scores:
                lines.append(f"- Convergence: {scores[0]:.3f} -> {scores[-1]:.3f}")
            coverage = convergence.get("map_elites_coverage", [])
            if coverage:
                lines.append(f"- MAP-Elites coverage: {coverage[0]} -> {coverage[-1]} cells")

        funnel = data.get("acceptance_funnel", {})
        if funnel and funnel.get("generations"):
            final_idx = len(funnel["generations"]) - 1
            lines.extend(["", "### Acceptance Funnel", ""])
            lines.append(f"- Final generation attempted: {funnel['attempted'][final_idx]}")
            lines.append(f"- Final generation accepted: {funnel['evaluated'][final_idx]}")
            lines.append(
                f"- Final generation acceptance rate: {funnel['acceptance_rate'][final_idx]:.3f}"
            )

        repair = data.get("repair_breakdown", {})
        if repair:
            lines.extend(["", "### Repair Breakdown", ""])
            lines.append(f"- Repair attempts: {repair.get('repair_attempts', 0)}")
            lines.append(f"- Repair successes: {repair.get('repair_successes', 0)}")
            lines.append(f"- Repair failures: {repair.get('repair_failures', 0)}")

        stat = baselines.get("stat_baselines", {})
        if isinstance(stat, dict) and stat:
            lines.extend(["", "### Baselines", ""])
            for bl_name, bl_data in stat.items():
                if isinstance(bl_data, dict):
                    bl_val = _get_spearman(bl_data.get("val_metrics"))
                    bl_test = _get_spearman(bl_data.get("test_metrics"))
                    val_str = f"{bl_val:.4f}" if bl_val is not None else "N/A"
                    test_str = f"{bl_test:.4f}" if bl_test is not None else "N/A"
                    lines.append(f"- {bl_name}: val={val_str}, test={test_str}")

        pysr_bl = baselines.get("pysr_baseline", {})
        if isinstance(pysr_bl, dict) and pysr_bl.get("status") == "ok":
            pysr_val = _get_spearman(pysr_bl.get("val_metrics"))
            pysr_test = _get_spearman(pysr_bl.get("test_metrics"))
            val_str = f"{pysr_val:.4f}" if pysr_val is not None else "N/A"
            test_str = f"{pysr_test:.4f}" if pysr_test is not None else "N/A"
            lines.append(f"- PySR: val={val_str}, test={test_str}")

        if ood:
            lines.extend(["", "### OOD Generalization", ""])
            for category, cat_data in ood.items():
                if isinstance(cat_data, dict):
                    ood_s = _get_spearman(cat_data)
                    ood_str = f"{ood_s:.4f}" if ood_s is not None else "N/A"
                    valid = cat_data.get("valid_count", "?")
                    total = cat_data.get("total_count", "?")
                    lines.append(f"- {category}: spearman={ood_str} ({valid}/{total} valid)")

        code = summary.get("best_candidate_code")
        if code:
            code = _normalize_candidate_code_for_report(code)
            lines.extend(["", "### Best Candidate Code", "", "```python", code, "```"])

        lines.append("")

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text("\n".join(lines) + "\n", encoding="utf-8")


def write_figure_data_json(
    experiments: dict[str, dict],
    output_path: Path,
    matrix_summaries: dict[str, dict[str, Any]] | None = None,
) -> dict[str, Any]:
    """Write JSON data for figure generation scripts."""
    figure_data: dict[str, Any] = {}
    for name, data in experiments.items():
        summary = data.get("summary", {})
        entry: dict[str, Any] = {
            "fitness_mode": summary.get("fitness_mode"),
            "success": summary.get("success"),
            "val_spearman": _get_spearman(summary.get("val_metrics")),
            "test_spearman": _get_spearman(summary.get("test_metrics")),
            "convergence": data.get("convergence", {}),
            "baselines": data.get("baselines", {}),
            "ood": data.get("ood", {}),
            "acceptance_funnel": data.get("acceptance_funnel", {}),
            "repair_breakdown": data.get("repair_breakdown", {}),
            "bounds_diagnostics": data.get("bounds_diagnostics", {}),
        }

        bounds = summary.get("bounds_metrics")
        if bounds:
            entry["bounds_metrics"] = bounds

        sc = summary.get("self_correction_stats")
        if sc:
            entry["self_correction_stats"] = sc

        runs = summary.get("runs")
        if runs:
            entry["runs"] = runs

        figure_data[name] = entry

    figure_data["__aggregates__"] = build_seed_aggregates(experiments)
    appendix_payload = build_appendix_payload(experiments, matrix_summaries or {})
    figure_data.update(appendix_payload)

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(figure_data, indent=2) + "\n", encoding="utf-8")
    return figure_data
