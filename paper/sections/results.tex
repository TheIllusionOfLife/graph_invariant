\section{Results}

\subsection{Cross-Experiment Comparison}

Table~\ref{tab:main_results} summarizes the main results across all four
experiments. The MAP-Elites ASPL experiment achieves the highest test Spearman
correlation ($\rho = 0.947$), meeting the success threshold of $\rho \geq 0.85$.
The algebraic connectivity experiment reaches $\rho = 0.778$, indicating that
this target is harder for the LLM to approximate from pre-computed features.
The upper-bound experiment achieves an 87\% satisfaction rate with a bound
score of 0.514, demonstrating that the system can find non-trivial empirical
inequalities on the evaluated graph distributions.

\begin{table}[t]
  \centering
  \caption{Summary of results across four experiment configurations. Spearman
    $\rho$ is reported on the validation (Val) and test sets. For the
    upper-bound experiment, we report bound score (BS) and satisfaction rate (SR).
    Benchmark reports mean $\pm$ std across 5 seeds. Success = (\#seeds with test $\rho \geq 0.85$) / total seeds.}
  \label{tab:main_results}
  \begin{tabular}{lccccc}
    \toprule
    Experiment & Mode & Gens & Val $\rho$ & Test $\rho$ & Success \\
    \midrule
    MAP-Elites ASPL     & correlation  & 30 & 0.935 & 0.947 & \checkmark \\
    Algebraic conn.     & correlation  & 20 & 0.765 & 0.778 & --- \\
    Upper bound ASPL    & upper\_bound & 20 & \multicolumn{2}{c}{BS=0.514, SR=87\%} & --- \\
    Benchmark (mean$\pm$std) & correlation & 20 & 0.927$\pm$0.011 & 0.921$\pm$0.027 & 5/5 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Baseline Comparison}

Table~\ref{tab:baselines} compares LLM-discovered formulas against statistical
and symbolic regression baselines on the ASPL target. The LLM formulas achieve
$\rho = 0.947$ on the test set, which trails PySR ($\rho = 0.975$),
linear regression ($\rho = 0.975$), and random forest ($\rho = 0.951$) by
2--3 percentage points. This gap reflects the cost of our composite objective,
which penalizes complexity and rewards novelty rather than optimizing
correlation alone. The strong linear regression performance ($\rho = 0.975$)
indicates that ASPL is well-approximated by linear combinations of graph
statistics; the LLM formulas trade predictive accuracy for interpretability
and structural insight.

\begin{table}[t]
  \centering
  \caption{Comparison of LLM-discovered formulas with baselines on
    average shortest path length. Val and Test Spearman $\rho$ reported.}
  \label{tab:baselines}
  \begin{tabular}{lcc}
    \toprule
    Method & Val $\rho$ & Test $\rho$ \\
    \midrule
    LLM (MAP-Elites)   & 0.935 & 0.947 \\
    LLM (Benchmark avg) & 0.927 & 0.921 \\
    PySR               & 0.982 & 0.975 \\
    Random Forest       & 0.961 & 0.951 \\
    Linear Regression   & 0.975 & 0.975 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Convergence Analysis}

Figure~\ref{fig:convergence} shows the evolution of the best validation score
across generations. All experiments exhibit a characteristic ``cold start''
in generations 0--1, where most candidates are rejected by the novelty gate
or sandbox. Acceptance rates increase from 5\% in generation~0 to over 74\%
by generation~4 in the MAP-Elites ASPL experiment, as the LLM learns the
sandbox constraints through the self-correction feedback loop. The MAP-Elites
ASPL experiment converges from a composite fitness score of 0.426 to 0.553
over 30 generations (note: these are weighted scores from Eq.~\ref{eq:score},
not raw Spearman $\rho$; the final validation $\rho = 0.935$ appears in
Table~\ref{tab:main_results}). The upper-bound experiment shows the steepest
relative improvement (0.228 $\to$ 0.453 composite score).

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/convergence.pdf}
  \caption{Convergence of best validation score across generations for each
    experiment. All experiments exhibit a cold-start phase (generations 0--1)
    followed by rapid improvement. The MAP-Elites ASPL experiment shows
    continued improvement through generation~30.}
  \label{fig:convergence}
\end{figure}

\subsection{MAP-Elites Archive Analysis}

Figure~\ref{fig:map_elites} shows the growth of the MAP-Elites archive over
generations. The archive grows from 2 occupied cells in generation~1 to 5 out
of 25 total cells (20\% coverage) by generation~30. While coverage is modest,
the quality-diversity archive prevents premature convergence: the best formula
emerged from a behavioral niche distinct from the initial high-scoring
candidates.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/map_elites_heatmap.pdf}
  \caption{MAP-Elites archive coverage over generations. Each cell in the
    $5 \times 5$ grid represents a behavioral niche defined by simplicity and
    novelty. Coverage grows from 2 to 5 cells over 30 generations.}
  \label{fig:map_elites}
\end{figure}

\subsection{Out-of-Distribution Generalization}

Figure~\ref{fig:ood} shows OOD Spearman correlations across the three
categories. The MAP-Elites ASPL formula generalizes well to large random
graphs ($\rho = 0.957$) and extreme-parameter graphs ($\rho = 0.926$), but
degrades on special topologies ($\rho = 0.500$) such as barbell and grid
graphs. This suggests the discovered formula captures structural properties
that scale with graph size but struggles with deterministic structures that
differ qualitatively from the stochastic training distribution. (The special-topology category contains 8 graphs; Spearman $\rho$ on 8 samples carries wide confidence intervals and should be interpreted as a preliminary signal, not a definitive estimate.)

\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/ood_generalization.pdf}
  \caption{Out-of-distribution generalization across three graph categories.
    Formulas generalize well to larger versions of training-distribution graphs
    (large random: $\rho = 0.957$) but degrade on qualitatively different
    topologies (special: $\rho = 0.500$).}
  \label{fig:ood}
\end{figure}

\subsection{Multi-Seed Benchmark Consistency}

Figure~\ref{fig:benchmark} shows the distribution of validation and test
Spearman correlations across 5 seeds. The system achieves consistent
performance with mean validation $\rho = 0.927 \pm 0.011$ and mean test
$\rho = 0.921 \pm 0.027$. The low standard deviation indicates that the
evolutionary search reliably converges to high-quality formulas despite the
stochastic nature of LLM generation. All five seeds meet the success threshold (test $\rho$ range: 0.873--0.953).

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/benchmark_boxplot.pdf}
  \caption{Distribution of Spearman $\rho$ across 5 benchmark seeds.
    Validation: $0.927 \pm 0.011$; Test: $0.921 \pm 0.027$. All five seeds meet the $\rho \geq 0.85$ threshold. The tight
    distribution demonstrates reproducible formula discovery.}
  \label{fig:benchmark}
\end{figure}

\subsection{Best Discovered Formulas}

Table~\ref{tab:formulas} presents the best-discovered formulas with
mathematical interpretation. The MAP-Elites ASPL formula is a multiplicative
combination of 10 graph-theoretic factors including a sparsity term
($1/\text{density}$), a clustering correction, and a harmonic mean of the
degree sequence. The upper-bound formula combines the path-graph bound
$(n+1)/3$ with Moore-bound-inspired terms and should be interpreted as an
empirically high-coverage bound in our testbed (not a universal proof).

\begin{table}[t]
  \centering
  \caption{Best discovered formulas per experiment with validation Spearman
    $\rho$ and key mathematical components.}
  \label{tab:formulas}
  \small
  \begin{tabular}{p{2.2cm}>{\raggedright\arraybackslash}p{6.5cm}c}
    \toprule
    Experiment & Key Formula Components & Val $\rho$ \\
    \midrule
    MAP-Elites ASPL & $\frac{\sqrt{n}}{d+1} \cdot \frac{1}{\delta} \cdot (1+C)^{0.6} \cdot \frac{d_H}{d} \cdot \ldots$ & 0.935 \\[4pt]
    Algebraic conn. & $\sqrt{n} \cdot \frac{d+1}{\sigma_d+1} \cdot (1+t^{0.25}) \cdot \sqrt{\delta} \cdot \frac{1}{1+C^{1.5}} \cdot \ldots$ & 0.765 \\[4pt]
    Upper bound & $\min\!\bigl(\frac{n+1}{3},\; d_\delta,\; r_\Delta,\; r_{\bar{d}}\bigr)$ (Moore bounds) & BS=0.514 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Self-Correction Effectiveness}

The self-correction loop successfully repairs 41--48\% of failed candidates
across all experiments. Specifically: MAP-Elites ASPL recovered 68 of 164
failures (41\%), algebraic connectivity recovered 36 of 75 (48\%), and upper
bound recovered 27 of 56 (48\%). The most common failure modes repaired are
sandbox violations (\texttt{import} statements) and novelty threshold
violations. Self-correction preserves the mathematical structure of the
original formula while fixing implementation issues, effectively acting as a
constrained search operator that retains mathematical intuition from failed
candidates.

\subsection{Day-1 Staged Pilot (Fast Profile)}

To evaluate a one-day iteration protocol, we ran a staged pilot with reduced
budgets (short generations/populations) and a faster local model profile, then
aggregated results over available seeds. Table~\ref{tab:day1_pilot} summarizes
the screening-stage outcomes from
\texttt{analysis/day1\_results/figure\_data.json}.

\begin{table}[t]
  \centering
  \caption{Day-1 staged pilot aggregates (fast profile). Values are mean $\pm$ std
    Spearman $\rho$ across available seeds.}
  \label{tab:day1_pilot}
  \begin{tabular}{lccc}
    \toprule
    Regime & Seeds & Val $\rho$ & Test $\rho$ \\
    \midrule
    MAP-Elites ASPL (screen) & 3 & $0.076 \pm 0.358$ & $0.142 \pm 0.493$ \\
    Small-data ASPL train=20 (screen) & 3 & $-0.015 \pm 0.275$ & $-0.054 \pm 0.268$ \\
    Upper-bound ASPL (screen) & 3 & $-0.072 \pm 0.167$ & $-0.067 \pm 0.268$ \\
    \bottomrule
  \end{tabular}
\end{table}

The pilot demonstrates that rapid screening is operationally feasible, but
performance is unstable under aggressive fast-profile settings. In particular,
the variance is large and mean performance does not meet our target thresholds.
This supports using fast-profile runs for pruning only, followed by
higher-budget confirmatory runs before locking paper claims.

For reviewer-facing reproducibility and uncertainty checks, Appendix
Tables~\ref{tab:appendix_seed_aggregates}--\ref{tab:appendix_runtime}
provide generated multi-seed aggregates, bounds diagnostics, and runtime
completion summaries directly from artifact summaries.
