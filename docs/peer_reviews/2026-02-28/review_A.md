# NeurIPS査読（Full Paper）

## 0. メタ情報
- **論文タイトル**: *Harmony-Driven Theory Discovery in Knowledge Graphs via LLM-Guided Island Search*
- **査読観点**: NeurIPS（フルペーパー）想定：新規性・技術妥当性・実験の説得力・再現性・影響・明確さ

---

## 1. まとめ（何をやったか）🧾
本論文は、科学領域の知識グラフ（KG）に対し、**「理論発見＝KGへの“変異（エッジ/エンティティの追加・削除）探索”」**として定式化し、
1) **複合評価指標 Harmony**（圧縮性・整合性・対称性・生成性）で KG 変異の良さをスコアリングし、
2) **LLM が“主張・根拠・反証条件”つきの提案（mutation）**を生成し、
3) **MAP-Elites（品質多様性）×島モデル探索（refine/combine/novel を循環）**で探索する枠組みを提案しています。

実験は、線形代数・周期表（キャリブレーション）と、天文・物理・材料（発見ドメイン）上の小規模 KG で行い、周波数ベースラインに対する Harmony スコア向上、DistMult のリンク予測指標（Hits@10 等）の改善、トップ提案の専門家ルーブリック評価（plausibility 平均≥3.0）を報告しています。

---

## 2. 強み（良い点）✅
1. **「トリプル予測」から「理論レベル提案」へ**という問題設定が明確
   - 単なる KGC のランキングではなく、**主張＋反証条件を必須**にしている点は、科学用途への姿勢として好ましいです。

2. **複合指標 Harmony の“解釈可能性”**
   - 4つのサブスコアに分解され、直感的に説明できる（圧縮性/整合性/対称性/生成性）。
   - さらに ablation で寄与の大小を示している点は丁寧です。

3. **探索設計（MAP-Elites + 島モデル）が筋が良い**
   - 探索対象が「言語で表現された提案」であり局所解に陥りやすいので、
     - novelty 島
     - refine 島
     - combine 島
     - 温度差
     - stagnation recovery
     を組み合わせた設計は合理的です。

4. **再現性に配慮した記述がある**
   - ハイパーパラメータ表、プロトコル、限界の明記、NeurIPS checklist の自己記入など。

---

## 3. 主要な懸念（採択判断に効く論点）⚠️
以下は「改善すれば強くなる」一方で、現状のままだと NeurIPS では突っ込まれやすいポイントです。

### 3.1 評価の“循環（circularity）”と主張の対応
- Harmony の **生成性（Generativity）が DistMult の Hits@K** で定義され、
- 実験の主な定量結果も **DistMult の Hits@10 等の改善**です。

つまり、**最適化目的に DistMult の指標が含まれており、評価も DistMult 指標**になっています。
この場合、改善が出ても「目的関数に含まれるものを最適化したので伸びました」に近く、
**“理論発見”としての外的妥当性（scientific validity）**を担保できているかが弱く見えます。

> 提案：DistMult 以外のモデル（RotatE 等）や、完全にモデル非依存な外部検証（後述）での“真の発見”評価が必要です。

### 3.2 「理論発見」と呼ぶには、実験ドメインが小さく、提案例も既知寄り
- KG が **17–30 entities / 30–80 edges** と非常に小さく、
- 提案例（例：質量光度関係、恒星核合成など）は、専門家には既知の範囲に見えます。

この規模だと、探索は「教科書知識の再提案」になりやすく、
**“発見”の主張は強すぎる**印象になり得ます。

> 提案：少なくとも「既知・未知の切り分け」や「既知事実の再発見（rediscovery）と新規仮説（novel hypothesis）」を分けて報告すると誤解を減らせます。

### 3.3 Harmony 指標の設計が“ゲーム可能”に見える箇所
- **Compressibility**: エッジ追加は一般に spanning fraction を下げ、エントロピーを変えます。
  - 「良い追加」がスコア上どう勝てるかの直感はある（tension を意図的に入れている）が、
  - 小規模 KG では些細な変化がスコアを大きく動かし、**局所的なスコアハック**の余地が見えます。

- **Coherence**: 三角形（triangle）整合性を使っていますが、
  - 小規模で疎な KG では triangle が少なく、信号が弱い。
  - さらに「rac ∈ {rab, rbc}」という“同型一致”は、関係の合成としては粗い。

- **Symmetry**: entity type ごとの outgoing edge type 分布の JS 距離。
  - 科学 KG はタイプごとに機能が異なるのが自然で、
    **“同じになること”が良い**という前提は疑義が出ます。

- **Contradicts のペナルティ**:
  - 科学知識では矛盾・対立説を explicit に保持することが価値な場合があり、
    contradicts 密度を「ノイズ」と見なすのはドメイン依存です。

> 提案：指標の各項が「いつ正当か／いつ不適切か」を、ドメイン別に議論し、代替定義や重み学習の可能性を示すと説得力が上がります。

### 3.4 ベースラインと評価プロトコルの“問いに対する整合性”
現状の評価は主に「提案を入れた後、KGC 指標が伸びるか」ですが、
理論発見としては、次がより本質的です：
- **提案エッジが真である確率（precision）**
- **既存文献で裏付けられるか（citation-supported）**
- **隠しておいた真のエッジをどれだけ当てたか（hit against withheld truths）**

また、DistMult-alone との比較は分かりやすい一方で、
「LLM が提案したものを Harmony で選別」する効果と
「単に正しいエッジを追加したら DistMult が良くなる」効果が混ざり、
**どこが効いたのかが切り分けづらい**です。

> 提案：
- (A) LLM 提案を **Harmony 以外のスコア**（例：単純に DistMult スコア上位）で選ぶ
- (B) Harmony のうち **Generativity を除いた版**で選ぶ（評価循環の解消）
- (C) LLM を使わず **プログラム的に候補生成**し Harmony で選別
などの因子分解実験があると強いです。

### 3.5 統計的有意性と頑健性
- 主結果が **single seed** で、誤差棒がありません。
- KG も小さいので分散が大きくなりがちです。

> 提案：最低でも 5 seed 程度、あるいは KG の構築揺らぎ（サブサンプリング）に対するロバスト性を示すべきです。

### 3.6 LLM 設定の再現性（モデル名・プロンプト・温度）
- Ollama ローカルという記述はあるものの、**具体的なモデル名（例：llama3:xxb 等）やバージョン**が明確でないと追試が難しいです。
- また、提案品質はプロンプトの影響が強いので、
  **最小限のプロンプトテンプレ**（匿名化した形でも）を appendix に入れると良いです。

---

## 4. 著者への質問（Rebuttal で答えると強くなる）❓
1. **評価循環の扱い**
   - Generativity に DistMult を使いつつ、評価も DistMult を使うことへの見解は？
   - Generativity を除いた Harmony（あるいは別モデル）でも同様の改善が出ますか？

2. **提案の“真偽”の直接評価**
   - 提案エッジのうち、どれくらいが withheld/backtesting set と一致しますか？
   - 一致しない場合、それは「新規仮説」としてどう検証可能ですか？

3. **小規模 KG からの一般化**
   - Wikidata サブグラフや既存の科学 KG（例えば材料系 KG）でのスケール実験は可能ですか？
   - 計算量（DistMult 再学習、Harmony 計算、LLM 呼び出し）はどこがボトルネックですか？

4. **Symmetry / contradicts ペナルティの妥当性**
   - “タイプ間で振る舞いが似るほど良い”という前提は、科学 KG で常に成り立つでしょうか？
   - contradicts を減らすことが、科学的知識表現において常に望ましいでしょうか？

5. **専門家評価の詳細**
   - 何名で、専門性は？ 単一評価者ならバイアスの懸念があります。
   - inter-annotator agreement を出せますか？

---

## 5. 改善提案（優先度順）🛠️
### 5.1 “理論発見”としての外部評価を追加（最重要）
- **文献検索（RAG）で裏取り**: 提案ごとに supporting / contradicting 文献を自動収集し、エビデンススコアを導入。
- **held-out truth と照合**: 生成した提案エッジが backtesting set とどれだけ一致するか（Precision@N）。
- **ヒト評価の拡張**: 少なくとも 2–3 人、評価基準の公開、合意度。

### 5.2 評価循環を断つ
- Generativity を DistMult 以外にする（例：RotatE、ComplEx などの別系統）
- あるいは評価指標を「DistMult 以外」に変更し、モデル依存を避ける

### 5.3 スケール実験
- 既存 KG のサブセット（数千ノード規模）で、
  - Harmony の計算コスト
  - 探索の収束
  - 提案の有用性
  を報告。

### 5.4 ベースラインの拡充
- LLM-only（Harmony なしで採択）
- Harmony-only（候補生成をプログラムで行う）
- QD なし（単純なビームサーチや MCTS など）

### 5.5 指標設計の妥当性の補強
- 各コンポーネントが「科学 KG で何を意味するか」を例で説明
- Symmetry の代替（タイプ間の“適切な差”を許容する指標）
- contradicts の扱い（矛盾をノイズではなく“多元的仮説”として表現する場合の設計）

---

## 6. 書き方・明確さ（軽微だが効く）✍️
1. **“Theory discovery”の定義を慎重に**
   - 現状は「仮説生成＋スコアリング」なので、発見というより“仮説探索支援”に近い。

2. **どの discovery domain が“best-performing”か明記**
   - Expert rubric の対象ドメインが本文から一意に分かると良いです。

3. **提案例をもっと出す**
   - 既知の再発見例と、（可能なら）新規仮説例を分ける。
   - “なぜその提案が Harmony を上げるのか”を分解して示すと納得感が増します。

---

## 7. 再現性・チェック（NeurIPS観点）🔁
- ✅ ハイパーパラメータ記載：十分
- ⚠️ LLM モデル名・バージョン・プロンプト：より明確化が必要
- ⚠️ 統計（seed / error bars）：主結果でも必要
- ✅ 計算資源：CPU-only で明確

---

## 8. Broader Impacts / 倫理🧭
- ポジティブ：仮説生成を加速し、学際的連結を見つける支援になり得ます。
- ネガティブ：もっともらしい誤情報の混入リスクは大きく、
  - 反証条件必須
  - 専門家ゲート
  の設計は良い方向です。
- 追加提案：
  - 提案を KG に“自動採用”しない運用（ステージング層）
  - 文献エビデンスを添付し、出典の不確実性を表示

---

## 9. 総合評価（スコア）🧮
**総合スコア（10点満点）: 6.5 / 10**
- **新規性**: 7/10（LLM + QD + KG 変異探索 + 複合指標の組み合わせは面白い）
- **技術妥当性**: 6/10（指標設計の前提と評価循環が懸念）
- **実験の説得力**: 5.5/10（小規模・single seed・外部真偽評価が不足）
- **明確さ**: 7/10（構成は読みやすく、提案スキーマも良い）
- **再現性**: 6.5/10（全体は丁寧だが LLM 詳細が足りない）

**推薦**: *Borderline / Weak Accept（ただし、Rebuttal で上記懸念への回答と追加実験が出せるなら）*

**Confidence（査読者の確信度, 5点満点）: 3.5 / 5**
- 理由：枠組みの意義は明確ですが、主張（theory discovery）に対して評価がやや間接で、追試・追加実験の有無で印象が大きく変わります。

