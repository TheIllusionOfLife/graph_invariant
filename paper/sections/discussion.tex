\section{Discussion}\label{sec:discussion}

\paragraph{Compressibility--generativity tension.}
Adding edges to a KG typically \emph{reduces} compressibility (the BFS spanning
fraction drops as cross-edges are introduced) while potentially \emph{improving}
generativity (more training signal for DistMult).  This tension is by design:
the Harmony metric rewards proposals that improve link-prediction learnability
without degrading structural simplicity.  The value function
(Eq.~\ref{eq:value}) with $\lambda > 0$ further penalises large mutations,
ensuring that only targeted, structurally justified proposals achieve high
scores.

\paragraph{Sparse KG challenges.}
Our curated KGs are deliberately small (17--30 entities, 30--80 edges) to
represent the early stages of scientific KG construction.  This sparsity limits
the generativity component: DistMult requires $\geq 10$ training edges to
produce meaningful predictions, and the 20\% masking protocol leaves few test
edges for evaluation.  Scaling to larger scientific KGs (e.g.\ Wikidata subsets)
would provide more statistical power for the generativity signal.

\paragraph{Proposal quality vs.\ validity rate.}
The stagnation recovery mechanism (constrained prompting after $S = 5$
generations without valid proposals) effectively maintains a validity rate
$\geq 0.50$ across domains.  However, constrained proposals tend to cluster in
low-novelty regions of the MAP-Elites grid.  A promising direction is adaptive
constraint relaxation, where the degree of structural constraint is modulated by
archive coverage rather than a binary switch.

\paragraph{Symmetry and contradicts validity.}
The symmetry component rewards entity-type behavioural uniformity, which may not
suit domains where entity types serve fundamentally different functional roles
(e.g.\ enzymes vs.\ substrates in biochemistry).  We acknowledge this limitation:
in functionally specialised domains, symmetry should receive lower weight or be
replaced by a type-aware variant that measures within-type consistency rather
than across-type uniformity.  Similarly, \texttt{contradicts} edges need not
represent noise---in scientific discourse, competing hypotheses are valuable and
their explicit representation is a feature, not a defect.  Our coherence penalty
targets only \emph{dense} contradiction (high contradicts-to-edge ratio), which
signals structural noise; sparse contradiction is tolerated.  Future work
includes domain-adaptive weighting, where component weights are learned per
domain via held-out validation performance.

\paragraph{LLM dependence and safety.}
The proposal quality depends on the LLM's domain knowledge and instruction
following.  Our experiments use a single model (\texttt{gpt-oss:20b});
ensembling across model families could improve diversity and robustness.  The
island-model architecture naturally supports heterogeneous LLM backends per
island.  To mitigate the risk of LLM-generated misinformation entering
scientific workflows, proposals enter a \emph{staging layer}: they are scored by
the Harmony metric and archived, but never automatically integrated into the
base KG.  Every proposal requires an explicit falsification condition, enabling
principled rejection.  Before any proposal is treated as established knowledge,
it must pass expert review---our rubric gate (mean plausibility $\geq 3.0$)
serves as a minimum quality filter, and we recommend domain-expert validation as
a mandatory step in any deployment.

\paragraph{Scalability.}
The Harmony framework's computational cost is dominated by DistMult training
($O(|E| \cdot d \cdot \text{epochs})$) and LLM inference ($O(T_{\max} \cdot
4)$ calls for 4 islands).  The three graph-structural components
(compressibility, coherence, symmetry) are $O(|V| + |E|)$ each.  For our
current KGs (17--22 entities), total wall time is $\sim$10 minutes per domain on
a single CPU.  Scaling to medium-size KGs (200--300 entities) increases DistMult
training time linearly with $|E|$ but does not change the LLM call count, making
the framework practical for KGs up to $\sim$1000 entities without GPU hardware.

\paragraph{Broader impacts.}
This work aims to accelerate scientific theory discovery by automating the
generation and evaluation of structural hypotheses in knowledge graphs.  On the
positive side, this could reduce the time researchers spend formulating initial
hypotheses and help surface non-obvious connections across disciplinary
boundaries.  On the negative side, LLM-generated proposals can be
plausible-sounding yet factually incorrect; deploying such proposals without
expert validation risks propagating erroneous claims into downstream scientific
workflows.  We mitigate this by including falsification conditions in every
proposal and requiring expert rubric scoring before any claim is treated as
established.

\paragraph{Limitations.}
(i)~The seven-relation type vocabulary, while sufficient for our five domains,
may be too coarse for highly specialised fields (e.g.\ organic chemistry
reaction types).
(ii)~Expert rubric evaluation is currently manual and limited to the top-5
proposals; automated plausibility scoring (e.g.\ via literature retrieval) would
improve scalability.
(iii)~The Harmony metric treats all edge types equally in the compressibility
and coherence components; domain-specific type hierarchies could improve these
signals.
(iv)~Although we report mean $\pm$ std across 10 seeds, the differences between
Harmony and DistMult-alone are not statistically significant on most domains
(paired $t$-test $p > 0.05$); larger-scale experiments with more generations or
bigger KGs may be needed to establish significance.
